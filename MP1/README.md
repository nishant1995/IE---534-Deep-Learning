# Neural Network Built from Scratch
Implemented a single layer neural network using 100 hidden units. Used a Rectified Linear Unit (ReLU) for activation. Trained the model over 20 epochs using Stochastic Gradient Decent and Backward Propagation Algorithm on the *MNIST* Dataset
